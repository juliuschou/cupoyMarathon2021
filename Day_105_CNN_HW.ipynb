{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 教學目標:\n",
    "    \n",
    "回顧 CNN 網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 範例說明:\n",
    "    \n",
    "使用 keras 預載的模型\n",
    "\n",
    "使用 keras VGG16 預訓練的權重\n",
    "\n",
    "了解預測後的結果輸出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業:\n",
    "\n",
    "    回答 Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "#載入預訓練模型\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    " # VGG 現存模型要找到一張名為elephant.jpg做處理的預設路徑\n",
    "img_path = 'elephant.jpg'\n",
    "#載入影像\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "#執行預測\n",
    "features = model.predict(x)\n",
    "print(features)\n",
    "# decode_predictions 輸出5個最高概率：(類別, 語義概念, 預測概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題:\n",
    "\n",
    "為什麼在CNNs中激活函數選用ReLU，而不用sigmoid或tanh函數？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **摘要**\n",
    "\n",
    "本文摘要於 B Chen 2021/12 Why Rectified Linear Unit (ReLU) in Deep Learning and the best practice to use it with TensorFlow的文章，先說明Sigmoid 與 Tanh 激活函數的問題，其次說明甚麼是ReLU，再從其實驗數據說明ReLU執行accuracy與速度皆優於 Sigmoid與Tanh。\n",
    "\n",
    "## **1. Sigmoid 與 Tanh 激活函數的問題**\n",
    "\n",
    "### **1. 1 Sigmoid 與 Tanh activation functions**\n",
    "\n",
    "![img](https://miro.medium.com/max/1400/1*ZRgbKGRAAqfKpBx3PATm_Q.png)\n",
    "\n",
    "Sigmoid activation function 1990年代很常被使用於神經網路(neural networks)，會將輸入值轉為0~1之間。\n",
    "\n",
    "Hyperbolic Tangent 也被稱為Tanh，是類似的nonlinear activation function，他的輸出值介於-1~1之間。在1990年代晚期與2000年代，神經網路模型較傾向使用Tanh，因為Tanh訓練與預測結果較好。\n",
    "\n",
    "### **1.2 梯度消失問題**\n",
    "\n",
    "![img](https://miro.medium.com/max/770/1*6GAXpaTLuHPo_gJBcAmDaQ.png)\n",
    "\n",
    "梯度消失是Sigmoid 與 Tanh常見的問題。從上圖可知當輸入值為極大或極小，Sigmoid 輸出值接近0或1，Tanh輸出值接近-1或1，微分的結果都接近於0。導致神經網路的反向傳遞沒有梯度下將的效果，後面的網路層就啟不了作用。這問題使得網路模型尤其是深度學習網路的學習效率低下。\n",
    "\n",
    "### **1.3 運算昂貴問題**\n",
    "\n",
    "Sigmoid 與 Tanh為指數型運算，因此它們另外一個問題是運算昂貴。\n",
    "\n",
    "## **2. 什麼是Rectified Linear Unit (ReLU)呢?**\n",
    "\n",
    "Rectified Linear Unit 最常被使用於深度學習。當輸入值小於等於0此函數回傳0，但輸入值大於0就直接回傳該值。函數定一如下:\n",
    "\n",
    "\n",
    "\n",
    "![img](https://miro.medium.com/max/310/1*3Jwh6c8Og8O0u5YiamF60w.png)\n",
    "\n",
    "![img](https://miro.medium.com/max/770/1*fWjpaGDDQQd5NxAVGAWBuw.png)\n",
    "\n",
    "從上圖可知:\n",
    "\n",
    "- 在ReLU中非線性的運算由兩個線性組合起來的。當函數的斜率不為常數，稱之為非線性函數。因此ReLU是接近0的非線性函數，他的斜率永遠是 0 (輸入值為負)或 1(輸入值為正)\n",
    "- ReLU 是連續的，當輸入值為負的是不可以微分的，因為它輸出值的變化率為0。\n",
    "- ReLU的輸出值沒有最大值(非飽和)，這是有助於梯度下降。\n",
    "- ReLU運算速度非常快(與 Sigmoid 跟 Tanh 比較)\n",
    "\n",
    "## **3. 比較ReLU 與 Sigmoid and Tanh 實驗數據**\n",
    "\n",
    "![img](https://miro.medium.com/max/770/1*v_uFJMLRoi6yMgWK7dHpfg.png)\n",
    "\n",
    "​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tB Chen 2021/1\n",
    "\n",
    "上圖為參考文件B Chen實驗數據呈現:\n",
    "\n",
    "- 使用ReLU激活函數中使用He initializer 可以得到最佳的 train accuracy 與 relu accuracy，次佳為tanh\n",
    "- 使用ReLU 與 Sigmoid 的訓練模型在training 與 validation的accuracy表現很差，兩個accuracy只達到約10%。意謂著這兩個模型沒辦法很好的找出資料的pattern，也無法很好的預測未知的資料。\n",
    "- 從下圖得知使用ReLU激活函數中使用He initializer執行時間平均每回合為33秒，相較於使用Tanh的模型平均每回合為 40秒。前者執行時間快了25%\n",
    "\n",
    "![img](https://miro.medium.com/max/1100/1*jirJB2n_dO1c2XbJMYc7xA.png)\n",
    "\n",
    "![img](https://miro.medium.com/max/1100/1*AXkv9TU0RF1Kl2EAd5ENDA.png)\n",
    "\n",
    "## 4. 結論\n",
    "\n",
    "Rectified 沒有梯度消失的問題，運算速度也比 Sigmoid 跟 Tanh快約25%，因此在CNNs中激活函數選用ReLU，而非Sigmoid 跟 Tanh。\n",
    "\n",
    "## **5. Reference**\n",
    "\n",
    "[Why Rectified Linear Unit (ReLU) in Deep Learning and the best practice to use it with TensorFlow](https://towardsdatascience.com/why-rectified-linear-unit-relu-in-deep-learning-and-the-best-practice-to-use-it-with-tensorflow-e9880933b7ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
