{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業重點:\n",
    "\n",
    "(1)以, Adam, 為例, 調整 batch_size, epoch , 觀察accurancy, loss 的變化\n",
    "\n",
    "(2)以同一模型, 分別驗證 SGD, Adam, Rmsprop 的 accurancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業目標:\n",
    "    \n",
    "    取得各種優化器的運算結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "from keras import optimizers\n",
    "import itertools\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "num_classes = 10\n",
    "data_augmentation = True\n",
    "num_predictions = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 資料正規化\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    第一步：選擇模型, 順序模型是多個網絡層的線性堆疊\n",
    " \n",
    "model = Sequential()\n",
    "\n",
    "#   第二步：構建網絡層\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense( 10)) # 輸出結果是10個類別，所以維度是10   \n",
    "model.add(Activation('softmax')) # 最後一層用softmax作為激活函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters：1250858\n"
     ]
    }
   ],
   "source": [
    "# 模型建立完成後，統計參數總量\n",
    "print(\"Total Parameters：%d\" % model.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 輸出模型摘要資訊\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)以, Adam, 為例, 調整 batch_size, epoch , 觀察accurancy, loss 的變化\n",
    "\n",
    "### (2)以同一模型, 分別驗證 SGD, Adam, Rmsprop 的 accurancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp: 0, batch_size=512, epochs: [20, 30], optimizer:SGD\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - 7s 30ms/step - loss: 2.2978 - accuracy: 0.1158 - val_loss: 2.2827 - val_accuracy: 0.1452\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 2.2498 - accuracy: 0.1518 - val_loss: 2.1821 - val_accuracy: 0.2175\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 2.1242 - accuracy: 0.2123 - val_loss: 2.0260 - val_accuracy: 0.2674\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 2.0395 - accuracy: 0.2432 - val_loss: 1.9793 - val_accuracy: 0.2822\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 1.9984 - accuracy: 0.2638 - val_loss: 1.9387 - val_accuracy: 0.3024\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 1.9627 - accuracy: 0.2830 - val_loss: 1.8907 - val_accuracy: 0.3259\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 1.9187 - accuracy: 0.3018 - val_loss: 1.8351 - val_accuracy: 0.3524\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 1.8631 - accuracy: 0.3226 - val_loss: 1.7725 - val_accuracy: 0.3755\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.7996 - accuracy: 0.3464 - val_loss: 1.7013 - val_accuracy: 0.4005\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.7441 - accuracy: 0.3611 - val_loss: 1.6421 - val_accuracy: 0.4135\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.6951 - accuracy: 0.3808 - val_loss: 1.6121 - val_accuracy: 0.4226\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 1.6578 - accuracy: 0.3916 - val_loss: 1.5628 - val_accuracy: 0.4355\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.6242 - accuracy: 0.4014 - val_loss: 1.5312 - val_accuracy: 0.4459\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.5932 - accuracy: 0.4178 - val_loss: 1.5179 - val_accuracy: 0.4517\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.5675 - accuracy: 0.4264 - val_loss: 1.4794 - val_accuracy: 0.4664\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.5379 - accuracy: 0.4415 - val_loss: 1.4623 - val_accuracy: 0.4695\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 1.5207 - accuracy: 0.4469 - val_loss: 1.4294 - val_accuracy: 0.4833\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.4970 - accuracy: 0.4574 - val_loss: 1.4071 - val_accuracy: 0.4935\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.4779 - accuracy: 0.4610 - val_loss: 1.3956 - val_accuracy: 0.4990\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.4571 - accuracy: 0.4719 - val_loss: 1.3746 - val_accuracy: 0.5071\n",
      "exp: 1, batch_size=512, epochs: [20, 30], optimizer:Adam\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - 3s 25ms/step - loss: 1.5388 - accuracy: 0.4540 - val_loss: 1.2856 - val_accuracy: 0.5470\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.2836 - accuracy: 0.5416 - val_loss: 1.1459 - val_accuracy: 0.5952\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.1728 - accuracy: 0.5852 - val_loss: 1.0671 - val_accuracy: 0.6243\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 1.0846 - accuracy: 0.6175 - val_loss: 0.9975 - val_accuracy: 0.6465\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 1.0224 - accuracy: 0.6401 - val_loss: 0.9288 - val_accuracy: 0.6778\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.9554 - accuracy: 0.6650 - val_loss: 0.8867 - val_accuracy: 0.6924\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.9032 - accuracy: 0.6833 - val_loss: 0.8502 - val_accuracy: 0.7037\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.8551 - accuracy: 0.7003 - val_loss: 0.8037 - val_accuracy: 0.7223\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.8112 - accuracy: 0.7159 - val_loss: 0.7847 - val_accuracy: 0.7277\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.7774 - accuracy: 0.7277 - val_loss: 0.7439 - val_accuracy: 0.7406\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.7240 - accuracy: 0.7466 - val_loss: 0.7227 - val_accuracy: 0.7529\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.6989 - accuracy: 0.7540 - val_loss: 0.7147 - val_accuracy: 0.7477\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.6780 - accuracy: 0.7613 - val_loss: 0.7007 - val_accuracy: 0.7588\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.6427 - accuracy: 0.7735 - val_loss: 0.6871 - val_accuracy: 0.7626\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.6036 - accuracy: 0.7871 - val_loss: 0.6738 - val_accuracy: 0.7686\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.5826 - accuracy: 0.7943 - val_loss: 0.6616 - val_accuracy: 0.7756\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.5620 - accuracy: 0.8019 - val_loss: 0.6464 - val_accuracy: 0.7761\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.5404 - accuracy: 0.8090 - val_loss: 0.6665 - val_accuracy: 0.7736\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.5167 - accuracy: 0.8173 - val_loss: 0.6495 - val_accuracy: 0.7780\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.4982 - accuracy: 0.8243 - val_loss: 0.6545 - val_accuracy: 0.7775\n",
      "exp: 2, batch_size=512, epochs: [20, 30], optimizer:RMSprop\n",
      "Epoch 1/20\n",
      "98/98 [==============================] - 3s 27ms/step - loss: 0.7914 - accuracy: 0.7649 - val_loss: 0.6733 - val_accuracy: 0.7689\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 2s 26ms/step - loss: 0.5914 - accuracy: 0.7928 - val_loss: 0.7441 - val_accuracy: 0.7524\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 2s 26ms/step - loss: 0.5694 - accuracy: 0.7999 - val_loss: 0.7209 - val_accuracy: 0.7623\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.5413 - accuracy: 0.8105 - val_loss: 0.6634 - val_accuracy: 0.7786\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.5268 - accuracy: 0.8142 - val_loss: 0.7560 - val_accuracy: 0.7476\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.5040 - accuracy: 0.8238 - val_loss: 0.7613 - val_accuracy: 0.7522\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4942 - accuracy: 0.8274 - val_loss: 0.6881 - val_accuracy: 0.7736\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4758 - accuracy: 0.8330 - val_loss: 0.7326 - val_accuracy: 0.7572\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4609 - accuracy: 0.8369 - val_loss: 0.6619 - val_accuracy: 0.7847\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4533 - accuracy: 0.8387 - val_loss: 0.6515 - val_accuracy: 0.7800\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4420 - accuracy: 0.8434 - val_loss: 0.6608 - val_accuracy: 0.7867\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4272 - accuracy: 0.8483 - val_loss: 0.6655 - val_accuracy: 0.7885\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4097 - accuracy: 0.8569 - val_loss: 0.7278 - val_accuracy: 0.7663\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.4070 - accuracy: 0.8564 - val_loss: 0.6841 - val_accuracy: 0.7833\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.3988 - accuracy: 0.8574 - val_loss: 0.6539 - val_accuracy: 0.7920\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.3942 - accuracy: 0.8615 - val_loss: 0.6585 - val_accuracy: 0.7928\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.3736 - accuracy: 0.8679 - val_loss: 0.6487 - val_accuracy: 0.7940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.3771 - accuracy: 0.8673 - val_loss: 0.6815 - val_accuracy: 0.7925\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.3588 - accuracy: 0.8712 - val_loss: 0.6722 - val_accuracy: 0.7925\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.3560 - accuracy: 0.8731 - val_loss: 0.7027 - val_accuracy: 0.7855\n",
      "exp: 3, batch_size=512, epochs: [20, 30], optimizer:SGD\n",
      "Epoch 1/30\n",
      "98/98 [==============================] - 3s 25ms/step - loss: 0.2822 - accuracy: 0.8991 - val_loss: 0.6463 - val_accuracy: 0.8027\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2693 - accuracy: 0.9046 - val_loss: 0.6470 - val_accuracy: 0.8040\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2615 - accuracy: 0.9060 - val_loss: 0.6514 - val_accuracy: 0.8024\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2610 - accuracy: 0.9059 - val_loss: 0.6603 - val_accuracy: 0.8053\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2545 - accuracy: 0.9106 - val_loss: 0.6587 - val_accuracy: 0.8030\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2607 - accuracy: 0.9054 - val_loss: 0.6518 - val_accuracy: 0.8053\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2534 - accuracy: 0.9089 - val_loss: 0.6566 - val_accuracy: 0.8053\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2552 - accuracy: 0.9095 - val_loss: 0.6526 - val_accuracy: 0.8058\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2501 - accuracy: 0.9119 - val_loss: 0.6579 - val_accuracy: 0.8039\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2499 - accuracy: 0.9120 - val_loss: 0.6548 - val_accuracy: 0.8062\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2490 - accuracy: 0.9110 - val_loss: 0.6588 - val_accuracy: 0.8057\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2448 - accuracy: 0.9126 - val_loss: 0.6590 - val_accuracy: 0.8079\n",
      "Epoch 13/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2427 - accuracy: 0.9128 - val_loss: 0.6548 - val_accuracy: 0.8080\n",
      "Epoch 14/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2426 - accuracy: 0.9147 - val_loss: 0.6604 - val_accuracy: 0.8058\n",
      "Epoch 15/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2471 - accuracy: 0.9122 - val_loss: 0.6598 - val_accuracy: 0.8049\n",
      "Epoch 16/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2438 - accuracy: 0.9130 - val_loss: 0.6608 - val_accuracy: 0.8066\n",
      "Epoch 17/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2443 - accuracy: 0.9139 - val_loss: 0.6544 - val_accuracy: 0.8064- loss: 0.2\n",
      "Epoch 18/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2424 - accuracy: 0.9120 - val_loss: 0.6629 - val_accuracy: 0.8051\n",
      "Epoch 19/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2425 - accuracy: 0.9147 - val_loss: 0.6621 - val_accuracy: 0.8060\n",
      "Epoch 20/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2438 - accuracy: 0.9131 - val_loss: 0.6587 - val_accuracy: 0.8074\n",
      "Epoch 21/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2365 - accuracy: 0.9154 - val_loss: 0.6675 - val_accuracy: 0.8066\n",
      "Epoch 22/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2352 - accuracy: 0.9163 - val_loss: 0.6603 - val_accuracy: 0.8066\n",
      "Epoch 23/30\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 0.2352 - accuracy: 0.9169 - val_loss: 0.6616 - val_accuracy: 0.8069\n",
      "Epoch 24/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2376 - accuracy: 0.9153 - val_loss: 0.6693 - val_accuracy: 0.8064\n",
      "Epoch 25/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2348 - accuracy: 0.9160 - val_loss: 0.6687 - val_accuracy: 0.8063\n",
      "Epoch 26/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2357 - accuracy: 0.9150 - val_loss: 0.6673 - val_accuracy: 0.8072\n",
      "Epoch 27/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2391 - accuracy: 0.9156 - val_loss: 0.6636 - val_accuracy: 0.8069\n",
      "Epoch 28/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2335 - accuracy: 0.9174 - val_loss: 0.6647 - val_accuracy: 0.8068\n",
      "Epoch 29/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2338 - accuracy: 0.9164 - val_loss: 0.6677 - val_accuracy: 0.8072\n",
      "Epoch 30/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2315 - accuracy: 0.9175 - val_loss: 0.6653 - val_accuracy: 0.8062\n",
      "exp: 4, batch_size=512, epochs: [20, 30], optimizer:Adam\n",
      "Epoch 1/30\n",
      "98/98 [==============================] - 3s 25ms/step - loss: 0.2882 - accuracy: 0.8962 - val_loss: 0.6525 - val_accuracy: 0.8006\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2889 - accuracy: 0.8966 - val_loss: 0.6516 - val_accuracy: 0.7999\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2869 - accuracy: 0.8968 - val_loss: 0.6401 - val_accuracy: 0.7996\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2746 - accuracy: 0.9005 - val_loss: 0.6578 - val_accuracy: 0.8044\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2712 - accuracy: 0.9035 - val_loss: 0.6537 - val_accuracy: 0.7997\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - 2s 22ms/step - loss: 0.2609 - accuracy: 0.9076 - val_loss: 0.6659 - val_accuracy: 0.8031\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2617 - accuracy: 0.9063 - val_loss: 0.6622 - val_accuracy: 0.8021\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2545 - accuracy: 0.9085 - val_loss: 0.6736 - val_accuracy: 0.8008\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2523 - accuracy: 0.9100 - val_loss: 0.6727 - val_accuracy: 0.8017\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2458 - accuracy: 0.9127 - val_loss: 0.6693 - val_accuracy: 0.8025\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2446 - accuracy: 0.9141 - val_loss: 0.6901 - val_accuracy: 0.8017\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2369 - accuracy: 0.9152 - val_loss: 0.6751 - val_accuracy: 0.8020\n",
      "Epoch 13/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2338 - accuracy: 0.9174 - val_loss: 0.6846 - val_accuracy: 0.8000\n",
      "Epoch 14/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2297 - accuracy: 0.9188 - val_loss: 0.6818 - val_accuracy: 0.8046\n",
      "Epoch 15/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2248 - accuracy: 0.9191 - val_loss: 0.6807 - val_accuracy: 0.8029\n",
      "Epoch 16/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2264 - accuracy: 0.9201 - val_loss: 0.6917 - val_accuracy: 0.8006\n",
      "Epoch 17/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2239 - accuracy: 0.9211 - val_loss: 0.6985 - val_accuracy: 0.8003\n",
      "Epoch 18/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2174 - accuracy: 0.9232 - val_loss: 0.7032 - val_accuracy: 0.8038\n",
      "Epoch 19/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2208 - accuracy: 0.9212 - val_loss: 0.6839 - val_accuracy: 0.8001\n",
      "Epoch 20/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2154 - accuracy: 0.9235 - val_loss: 0.6833 - val_accuracy: 0.8071\n",
      "Epoch 21/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2144 - accuracy: 0.9234 - val_loss: 0.6946 - val_accuracy: 0.8021\n",
      "Epoch 22/30\n",
      "98/98 [==============================] - 3s 27ms/step - loss: 0.2075 - accuracy: 0.9267 - val_loss: 0.6794 - val_accuracy: 0.8047\n",
      "Epoch 23/30\n",
      "98/98 [==============================] - 3s 30ms/step - loss: 0.2083 - accuracy: 0.9256 - val_loss: 0.6967 - val_accuracy: 0.8017\n",
      "Epoch 24/30\n",
      "98/98 [==============================] - 3s 27ms/step - loss: 0.2029 - accuracy: 0.9293 - val_loss: 0.6797 - val_accuracy: 0.8078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1958 - accuracy: 0.9295 - val_loss: 0.7115 - val_accuracy: 0.8048\n",
      "Epoch 26/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2042 - accuracy: 0.9276 - val_loss: 0.6923 - val_accuracy: 0.8061\n",
      "Epoch 27/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.1954 - accuracy: 0.9301 - val_loss: 0.7154 - val_accuracy: 0.8068\n",
      "Epoch 28/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.1958 - accuracy: 0.9308 - val_loss: 0.6965 - val_accuracy: 0.8018\n",
      "Epoch 29/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.2003 - accuracy: 0.9290 - val_loss: 0.7136 - val_accuracy: 0.8017\n",
      "Epoch 30/30\n",
      "98/98 [==============================] - 2s 23ms/step - loss: 0.1888 - accuracy: 0.9323 - val_loss: 0.7210 - val_accuracy: 0.8029\n",
      "exp: 5, batch_size=512, epochs: [20, 30], optimizer:RMSprop\n",
      "Epoch 1/30\n",
      "98/98 [==============================] - 3s 27ms/step - loss: 0.2336 - accuracy: 0.9188 - val_loss: 0.7402 - val_accuracy: 0.7964\n",
      "Epoch 2/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2210 - accuracy: 0.9234 - val_loss: 0.7642 - val_accuracy: 0.7994\n",
      "Epoch 3/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2297 - accuracy: 0.9212 - val_loss: 0.6951 - val_accuracy: 0.8044\n",
      "Epoch 4/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2168 - accuracy: 0.9241 - val_loss: 0.7458 - val_accuracy: 0.7998\n",
      "Epoch 5/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2143 - accuracy: 0.9251 - val_loss: 0.7253 - val_accuracy: 0.8049\n",
      "Epoch 6/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2183 - accuracy: 0.9227 - val_loss: 0.7328 - val_accuracy: 0.7995\n",
      "Epoch 7/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2164 - accuracy: 0.9233 - val_loss: 0.7481 - val_accuracy: 0.7955\n",
      "Epoch 8/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2073 - accuracy: 0.9258 - val_loss: 0.7493 - val_accuracy: 0.7978\n",
      "Epoch 9/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2087 - accuracy: 0.9262 - val_loss: 0.7995 - val_accuracy: 0.7983\n",
      "Epoch 10/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2050 - accuracy: 0.9286 - val_loss: 0.7492 - val_accuracy: 0.8028\n",
      "Epoch 11/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2040 - accuracy: 0.9288 - val_loss: 0.8046 - val_accuracy: 0.8029\n",
      "Epoch 12/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2039 - accuracy: 0.9290 - val_loss: 0.7831 - val_accuracy: 0.7997\n",
      "Epoch 13/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2098 - accuracy: 0.9263 - val_loss: 0.7153 - val_accuracy: 0.8069\n",
      "Epoch 14/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2013 - accuracy: 0.9293 - val_loss: 0.7513 - val_accuracy: 0.7926\n",
      "Epoch 15/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2064 - accuracy: 0.9274 - val_loss: 0.7222 - val_accuracy: 0.7960\n",
      "Epoch 16/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.2040 - accuracy: 0.9286 - val_loss: 0.7609 - val_accuracy: 0.8041\n",
      "Epoch 17/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1986 - accuracy: 0.9316 - val_loss: 0.7214 - val_accuracy: 0.8090\n",
      "Epoch 18/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1942 - accuracy: 0.9316 - val_loss: 0.7900 - val_accuracy: 0.7988\n",
      "Epoch 19/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.1975 - accuracy: 0.9300 - val_loss: 0.7408 - val_accuracy: 0.8068\n",
      "Epoch 20/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1955 - accuracy: 0.9312 - val_loss: 0.7681 - val_accuracy: 0.8051\n",
      "Epoch 21/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1916 - accuracy: 0.9319 - val_loss: 0.8198 - val_accuracy: 0.7998\n",
      "Epoch 22/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.2013 - accuracy: 0.9302 - val_loss: 0.7500 - val_accuracy: 0.7960\n",
      "Epoch 23/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1858 - accuracy: 0.9350 - val_loss: 0.7648 - val_accuracy: 0.8043\n",
      "Epoch 24/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1948 - accuracy: 0.9326 - val_loss: 0.8158 - val_accuracy: 0.8003\n",
      "Epoch 25/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.1928 - accuracy: 0.9332 - val_loss: 0.7505 - val_accuracy: 0.8053\n",
      "Epoch 26/30\n",
      "98/98 [==============================] - 2s 24ms/step - loss: 0.1914 - accuracy: 0.9338 - val_loss: 0.7466 - val_accuracy: 0.8085\n",
      "Epoch 27/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1920 - accuracy: 0.9328 - val_loss: 0.7335 - val_accuracy: 0.8072\n",
      "Epoch 28/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1959 - accuracy: 0.9317 - val_loss: 0.7473 - val_accuracy: 0.8081\n",
      "Epoch 29/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1907 - accuracy: 0.9342 - val_loss: 0.7527 - val_accuracy: 0.8047\n",
      "Epoch 30/30\n",
      "98/98 [==============================] - 2s 25ms/step - loss: 0.1956 - accuracy: 0.9338 - val_loss: 0.8575 - val_accuracy: 0.7785\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epoch</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>train-loss</th>\n",
       "      <th>valid-loss</th>\n",
       "      <th>train-acc</th>\n",
       "      <th>valid-acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>SGD</td>\n",
       "      <td>[2.297805070877075, 2.249804973602295, 2.12421...</td>\n",
       "      <td>[2.2827370166778564, 2.182128667831421, 2.0260...</td>\n",
       "      <td>[0.11584000289440155, 0.15177999436855316, 0.2...</td>\n",
       "      <td>[0.1451999992132187, 0.2175000011920929, 0.267...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>Adam</td>\n",
       "      <td>[1.5388472080230713, 1.2835760116577148, 1.172...</td>\n",
       "      <td>[1.2855613231658936, 1.1459367275238037, 1.067...</td>\n",
       "      <td>[0.4539799988269806, 0.5415800213813782, 0.585...</td>\n",
       "      <td>[0.546999990940094, 0.5952000021934509, 0.6243...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>512</td>\n",
       "      <td>20</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>[0.7913756370544434, 0.5913901329040527, 0.569...</td>\n",
       "      <td>[0.6732553243637085, 0.7440592050552368, 0.720...</td>\n",
       "      <td>[0.7649199962615967, 0.7927799820899963, 0.799...</td>\n",
       "      <td>[0.7688999772071838, 0.7523999810218811, 0.762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>SGD</td>\n",
       "      <td>[0.2821560800075531, 0.26928815245628357, 0.26...</td>\n",
       "      <td>[0.6462984681129456, 0.6469695568084717, 0.651...</td>\n",
       "      <td>[0.8991199731826782, 0.9046199917793274, 0.905...</td>\n",
       "      <td>[0.8026999831199646, 0.8040000200271606, 0.802...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>Adam</td>\n",
       "      <td>[0.2882141172885895, 0.2889169752597809, 0.286...</td>\n",
       "      <td>[0.6524515151977539, 0.6515591740608215, 0.640...</td>\n",
       "      <td>[0.8961600065231323, 0.896619975566864, 0.8967...</td>\n",
       "      <td>[0.800599992275238, 0.7998999953269958, 0.7996...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>512</td>\n",
       "      <td>30</td>\n",
       "      <td>RMSprop</td>\n",
       "      <td>[0.23363080620765686, 0.2209729701280594, 0.22...</td>\n",
       "      <td>[0.7401626110076904, 0.7642381191253662, 0.695...</td>\n",
       "      <td>[0.9188399910926819, 0.9233800172805786, 0.921...</td>\n",
       "      <td>[0.7964000105857849, 0.7993999719619751, 0.804...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exp  batch_size  epoch optimizer  \\\n",
       "0    0         512     20       SGD   \n",
       "1    1         512     20      Adam   \n",
       "2    2         512     20   RMSprop   \n",
       "3    3         512     30       SGD   \n",
       "4    4         512     30      Adam   \n",
       "5    5         512     30   RMSprop   \n",
       "\n",
       "                                          train-loss  \\\n",
       "0  [2.297805070877075, 2.249804973602295, 2.12421...   \n",
       "1  [1.5388472080230713, 1.2835760116577148, 1.172...   \n",
       "2  [0.7913756370544434, 0.5913901329040527, 0.569...   \n",
       "3  [0.2821560800075531, 0.26928815245628357, 0.26...   \n",
       "4  [0.2882141172885895, 0.2889169752597809, 0.286...   \n",
       "5  [0.23363080620765686, 0.2209729701280594, 0.22...   \n",
       "\n",
       "                                          valid-loss  \\\n",
       "0  [2.2827370166778564, 2.182128667831421, 2.0260...   \n",
       "1  [1.2855613231658936, 1.1459367275238037, 1.067...   \n",
       "2  [0.6732553243637085, 0.7440592050552368, 0.720...   \n",
       "3  [0.6462984681129456, 0.6469695568084717, 0.651...   \n",
       "4  [0.6524515151977539, 0.6515591740608215, 0.640...   \n",
       "5  [0.7401626110076904, 0.7642381191253662, 0.695...   \n",
       "\n",
       "                                           train-acc  \\\n",
       "0  [0.11584000289440155, 0.15177999436855316, 0.2...   \n",
       "1  [0.4539799988269806, 0.5415800213813782, 0.585...   \n",
       "2  [0.7649199962615967, 0.7927799820899963, 0.799...   \n",
       "3  [0.8991199731826782, 0.9046199917793274, 0.905...   \n",
       "4  [0.8961600065231323, 0.896619975566864, 0.8967...   \n",
       "5  [0.9188399910926819, 0.9233800172805786, 0.921...   \n",
       "\n",
       "                                           valid-acc  \n",
       "0  [0.1451999992132187, 0.2175000011920929, 0.267...  \n",
       "1  [0.546999990940094, 0.5952000021934509, 0.6243...  \n",
       "2  [0.7688999772071838, 0.7523999810218811, 0.762...  \n",
       "3  [0.8026999831199646, 0.8040000200271606, 0.802...  \n",
       "4  [0.800599992275238, 0.7998999953269958, 0.7996...  \n",
       "5  [0.7964000105857849, 0.7993999719619751, 0.804...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sizes = [512]\n",
    "epochs = [20, 30]\n",
    "optimizer_set = [keras.optimizers.gradient_descent_v2.SGD(learning_rate=LEARNING_RATE, nesterov=True, momentum=0.95),\n",
    "                 tensorflow.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                 tensorflow.keras.optimizers.RMSprop(learning_rate=LEARNING_RATE)]\n",
    "\n",
    "results = []\n",
    "for i, (batch_size, epoch, opt) in enumerate(itertools.product(batch_sizes, epochs, optimizer_set)):\n",
    "    print(f\"exp: {i}, batch_size={batch_size}, epochs: {epochs}, optimizer:{type(opt).__name__}\")\n",
    "    \n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "    history=model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epoch,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)       \n",
    "    \n",
    "    results.append( {'exp': i,\n",
    "                     'batch_size': batch_size,\n",
    "                     'epoch': epoch,\n",
    "                     'optimizer': type(opt).__name__,\n",
    "                     'train-loss': model.history.history[\"loss\"],\n",
    "                     'valid-loss': model.history.history[\"val_loss\"],\n",
    "                     'train-acc': model.history.history[\"accuracy\"],\n",
    "                     'valid-acc': model.history.history[\"val_accuracy\"],\n",
    "                    })    \n",
    "        \n",
    "\n",
    "experiments = pd.DataFrame(results, columns =['exp', 'batch_size', 'epoch', 'optimizer', 'train-loss', 'valid-loss', 'train-acc', 'valid-acc'])\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "columns = ['loss_score', 'accuracy_score', 'optimizer', 'loss_function', 'epoch', 'learning_rate', 'decacy', \\\n",
    "           'momentum']\n",
    "loss_accuracy_sore = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adm1 = load_model(save_dir+\"/keras_cifar10_opt_adm_epochs_20.h5\")\n",
    "scores_adm1 = model.evaluate(x_test,y_test,batch_size=200,verbose= 0)\n",
    "loss_accuracy_sore.append(['', scores_adm1, 'adm', 'categorical_crossentropy', 20, '', '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_RMSprop = load_model(save_dir+\"/keras_cifar10_opt_RMSprop_epochs_30_lr_0.01-v1r01.h5\")\n",
    "scores_RMSprop = model.evaluate(x_test,y_test,batch_size=200,verbose= 0)\n",
    "loss_accuracy_sore.append(['', scores_RMSprop, 'RMSprop', 'categorical_crossentropy', 30, '', '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_accuracy_sore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    第六步：輸出\n",
    "import numpy \n",
    "\n",
    "print ( \" test set \" )\n",
    "scores = model.evaluate(x_test,y_test,batch_size=200,verbose= 0)\n",
    "print ( \"\" )\n",
    "#print ( \" The test loss is %f \" % scores)\n",
    "print ( \" The test loss is %f \", scores)\n",
    "\n",
    "\n",
    "result = model.predict(x_test,batch_size=200,verbose= 0)\n",
    "\n",
    "result_max = numpy.argmax(result, axis = 1 )\n",
    "test_max = numpy.argmax(y_test, axis = 1 )\n",
    "\n",
    "result_bool = numpy.equal(result_max, test_max)\n",
    "true_num = numpy.sum(result_bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Valiidation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Valiidation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
